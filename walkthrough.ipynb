{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Fullstack AI Image Generation Web App with Stable Diffusion, Fast API and React"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a fun project for a hot summy weekend, I built a minimal fullstack text-to-image AI web app with [Stable Diffusion](https://stability.ai/stablediffusion) deployed through [Amazon SageMaker JumpStart](https://aws.amazon.com/sagemaker/jumpstart), [FastAPI](https://fastapi.tiangolo.com/) for the web backend, and [React](https://react.dev/) for the web frontend. \n",
    "\n",
    "Here I'm walking through how I built it step by step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the Stable Diffusion model, I took a shortcut to use SageMaker JumpStart: `Stable Diffusion 2.1 base` pretrained on [LAION-5B](https://laion.ai/blog/laion-5b/) is provided under SageMaker JumpStart's \"Foundation Models: Image Generation\" ML task. It can be deployed by one click, so that's what I did!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the model:\n",
    "\n",
    "![SageMaker JumpStart](images/SageMaker-JumpStart.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-click deploy it (here I used all the default settings for simplicity):\n",
    "![Deploy](images/Model-deploy.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the endpoint is ready in service, a sample notebook is provided.\n",
    "![open notebook](images/Open-notebook.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By opening it in SageMaker Studio you can already play around with it. Since I have all my AWS credentials and Jupyter notebook server already set up locally, I downloaded it and had some fun in my local VSCode ðŸ˜†. (You can find this notebook in my github repo.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# response = query_endpoint(\"cottage in impressionist style\")\n",
    "response = query_endpoint(\"a cat astronaut fighting aliens in space, realistic, high res\")\n",
    "\n",
    "img, prmpt = parse_response(response)\n",
    "\n",
    "# Display hallucinated image\n",
    "display_image(img,prmpt)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/output-1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Fast and Minimum Start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've wanted to try out FastAPI because it looks so fast and has a nice Swagger experience built in. So I first installed it as instructed in the [official website](https://fastapi.tiangolo.com/#installation):\n",
    "\n",
    "```bash\n",
    "# Install fastapi as well as the ASGI server Uvicorn\n",
    "$ pip3 install fastapi\n",
    "$ pip3 install uvicorn[standard]\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I created a `main.py` file under my `api/` folder and typed in the minimal example of a FastAPI app:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from typing import Union\n",
    "\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"Hello\": \"World\"}\n",
    "\n",
    "\n",
    "@app.get(\"/items/{item_id}\")\n",
    "def read_item(item_id: int, q: Union[str, None] = None):\n",
    "    return {\"item_id\": item_id, \"q\": q}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `uvicorn` dev server in my `/api` folder:\n",
    "\n",
    "```bash\n",
    "$ uvicorn main:app --reload\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as promised, I got a working API at `http://127.0.0.1L8000/` and my API doc Swagger UI at `http://127.0.0.1:8000/docs`, instantly ðŸš€!\n",
    "\n",
    "![](images/fast-api.png)\n",
    "\n",
    "![](images/swagger.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect the API to the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to make our API able to send a prompt to our Stable Diffusion model and get a response image. Let me first add a route and handler function for that:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# api/main.py\n",
    "@app.get(\"/generate-image\")\n",
    "def generate_image(prompt: str):\n",
    "    image, prmpt = utils.parse_response(utils.query_endpoint(prompt))\n",
    "    print(image)\n",
    "    return {\"out\": \"yeah\"}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I picked the `query_endpoint()` function and the `parse_response()` function from the Stable Diffusion example Notebook that SageMaker JumpStart generated for me, and packed them in a `utils.py` file to be used in the API `main.py` file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the `main.py` file with the added new route, the Swagger UI conveniently added it and provides a test UI for me to input my prompt!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/swagger-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the model does send back the image generation! What does it look like? Well, it's an array of RGB channel values of each pixel in the image! Crazy, isn't it?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/image-pixels.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Save the Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn the magic numbers into image, I used [numpy](https://numpy.org/) and [PIL(Pillow)](https://pypi.org/project/Pillow/). For a quick test, I just added another utility function to save the image to disk (where the function is running)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# api/utils.py\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# ...\n",
    "\n",
    "def save_image(pixels):\n",
    "    arr = np.array(pixels, dtype=np.uint8)\n",
    "    img = Image.fromarray(arr)\n",
    "    img.save(\"new.png\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then in the API `/generate-image` route's handler, I plugged the image pixel array from response to this new utility function, and sent the prompt as the API response for now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# api/main.py\n",
    "# ...\n",
    "@app.get(\"/generate-image\")\n",
    "def generate_image(prompt: str):\n",
    "    image, prmpt = utils.parse_response(utils.query_endpoint(prompt))\n",
    "    utils.save_image(image)\n",
    "    return {\"prompt\": prmpt}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested out in the Swagger UI with a new prompt: \"A unicorn astronaut in space, full body from side\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/swagger-3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the `/api` folder a `new.png` appeared - the application works! Have a look at the unicorn astronaut image generated by Stable Diffusion and saved by the application:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/new.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support CORS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the UI to make AJAX call to the API later, I need to enable CORS support for the API. To do that, I added FastAPI's [CORSMiddleware](https://fastapi.tiangolo.com/tutorial/cors/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# api/main.py\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "# Support CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the UI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaffolding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the image generation backend is working, I'm marching on to build the frontend. For speed and simplicity, I used [Vite](https://vitejs.dev/), choosing the `React + TypeScript + SWC` variant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/vite-ui.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For UI components, I wanted to try out [Chakra](https://chakra-ui.com/) for a long time. It has a nice [installation guide](https://chakra-ui.com/getting-started/vite-guide) when using Vite.\n",
    "\n",
    "```bash\n",
    "$ npm i @chakra-ui/react @emotion/react @emotion/styled framer-motion\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installation, just sneak in the `<ChakraProvider>` tags into the `main.tsx` code:\n",
    "\n",
    "```tsx\n",
    "// ui/src/main.tsx\n",
    "import { ChakraProvider } from \"@chakra-ui/react\";\n",
    "\n",
    "ReactDOM.createRoot(document.getElementById(\"root\")!).render(\n",
    "  <React.StrictMode>\n",
    "    <ChakraProvider>\n",
    "      <App />\n",
    "    </ChakraProvider>\n",
    "  </React.StrictMode>\n",
    ");\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Minimal UI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimal UI would just need three components: \n",
    "1. a text input for the user to enter the prompt\n",
    "2. a submit button to send the prompt to the API\n",
    "3. an image component to show the generated image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Chakra-UI's ready-made components `Input`, `Button`, and `Image`, it's much easier to make these look properly nice. For layout I used a `Container`, an `InputGroup`, and tweaked some margins here and there. For completeness I also added a `Heading`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tsx\n",
    "// ui/src/App.tsx\n",
    "// ...\n",
    "\n",
    "return (\n",
    "    <Container maxWidth={\"2xl\"} marginTop={30} centerContent>\n",
    "      <Heading margin={8}>Image Generator</Heading>\n",
    "      <InputGroup>\n",
    "        <Input\n",
    "          pr=\"4.5rem\"\n",
    "          value={prompt}\n",
    "          placeholder=\"Enter your prompt\"\n",
    "          onChange={onInputChange}\n",
    "        />\n",
    "        <InputRightElement width={\"6rem\"}>\n",
    "          <Button onClick={onButtonClick} isDisabled={isLoading}>\n",
    "            Generate!\n",
    "          </Button>\n",
    "        </InputRightElement>\n",
    "      </InputGroup>\n",
    "      {imgSrc ? (\n",
    "        <Box boxSize={\"l\"} marginTop={5}>\n",
    "          <Image src=\"https://picsum.photos/640\" />\n",
    "        </Box>\n",
    "      ) : null}\n",
    "    </Container> \n",
    ");\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tweak the image size and position I used a dynamic dummy image  from https://picsum.photos/.\n",
    "\n",
    "![](images/minimal-ui.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UI States, Event Handlers, API Call"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three pieces of dynamic states in the minimal UI app: \n",
    "1. the `prompt` text, which is bound to the value of our text input.\n",
    "2. the `imageSrc`, which should come from the backend and also dictate whether to render the image component at all. \n",
    "3. a `isLoading` state, for a better user experience I'd like to show a loader and disable the \"Generate\" button when the user is waiting for their image to be generated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```typescript\n",
    "// ui/src/App.tsx\n",
    "function App() {\n",
    "  // states\n",
    "  const [prompt, setPrompt] = useState(\"\");\n",
    "  const [isLoading, setIsLoading] = useState(false);\n",
    "  const [imgSrc, setImgSrc] = useState(\"https://picsum.photos/640\"); // we'll replace the initial value to null later so that the image component is not rendered when there is no imgSrc\n",
    "//...\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for event handlers, first bind the text input's change event to update the prompt state with every change of the input value:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tsx\n",
    "// ui/src/App.tsx\n",
    "  const onInputChange = (e: ChangeEvent<HTMLInputElement>) => {\n",
    "    setPrompt(e.target.value);\n",
    "  };\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a \"Generate\" button click should trigger an AJAX fetch call to the `/generate-image?prompt=` API, with the `prompt` state value given as the query parameter. I wrapped the API call with some careful error handling, as well as updating the loading states for UX:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tsx\n",
    "// ui/src/App.tsx\n",
    "const onButtonClick = async () => {\n",
    "    setIsLoading(true);\n",
    "    try {\n",
    "      const response = await (\n",
    "        await fetch(`http://127.0.0.1:8000/generate-image?prompt=${prompt}`)\n",
    "      ).json();\n",
    "      console.log(response);\n",
    "    } catch (err: any) {\n",
    "      // catch any runtime error\n",
    "      console.log(err.message);\n",
    "    } finally {\n",
    "      setIsLoading(false);\n",
    "    }\n",
    "};\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enhance the user experience, let me add a spinner below the input:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tsx\n",
    "// ui/src/App.tsx\n",
    "// ...\n",
    "    </InputGroup>    \n",
    "    {isLoading ? (\n",
    "        <Spinner\n",
    "          thickness=\"4px\"\n",
    "          speed=\"0.65s\"\n",
    "          emptyColor=\"gray.200\"\n",
    "          color=\"blue.500\"\n",
    "          size=\"xl\"\n",
    "          marginTop={6}\n",
    "        />\n",
    "      ) : imgSrc ? (\n",
    "        <Box boxSize={\"l\"} marginTop={5}>\n",
    "          {/* <Image src={`data:image/png;base64,${imageData}`} /> */}\n",
    "          <Image src={imgSrc} />\n",
    "        </Box>\n",
    "      ) : null}\n",
    "  </Container>\n",
    "//...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, entering a prompt and clicking the \"Generate!\" button actually triggers the image generation API and makes an inference call to the Stable Diffusion model. To see the generated image, just check the `new.png` image in the `/api` folder!  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to the prompt \"A fluffy rabbit hacker coding at a macbook pro, cyberpunk\", this image was saved in the `/api` folder:\n",
    "\n",
    "![](images/new-rabbit-hacker.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, instead of saving the image locally, our API is going to send it back the front end!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backend and Frontend Integration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the exciting part, our user will finally see the generated image after clicking the \"Generate!\" button!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Image In the API Response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, our API has only been sending the prompt back, while saving the image for itself. \n",
    "\n",
    "To get the image out of it, we'll send the image data as a base64 encoded string. So our API response JSON schema becomes:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": prmpt, // the prompt string from user \n",
    "    \"img_base64\": img_str // the generated image as a base64 data string\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd still want to keep a copy of the generated image in the server, so I rearranged the image related utility functions to be\n",
    "\n",
    "1. a general util function to convert the pixel arrays from Stable Diffusion into a PIL Image\n",
    "   \n",
    "   ```python\n",
    "   # utils.py\n",
    "   def pixel_to_image(pixel_array):\n",
    "       arr = np.array(pixel_array, dtype=np.uint8)\n",
    "       img = Image.fromarray(arr)\n",
    "       return img\n",
    "   ```\n",
    "2. a function to save a PIL image to the server\n",
    "   \n",
    "   ```python\n",
    "   # utils.py\n",
    "   def save_image(img, filePath=\"generated_images/new.png\"):\n",
    "       img.save(filePath)\n",
    "   ```\n",
    "   This one accepts a file path, and by default it will save the image as `new.png` in the `generated_images` folder. \n",
    "\n",
    "3. a function to convert a PIL image to a base64 encoded image data string\n",
    "   \n",
    "   ```python\n",
    "   # utils.py\n",
    "   def image_to_base64_str(img, format=\"PNG\"):\n",
    "       buffered = BytesIO()\n",
    "       img.save(buffered, format=format)\n",
    "       img_str = base64.b64encode(buffered.getvalue())\n",
    "       return img_str\n",
    "    ```\n",
    "\n",
    "Then in our API app, using these utils functions to send back both the prompt and the image data string is easy:\n",
    "\n",
    "```python\n",
    "# main.py\n",
    "@app.get(\"/generate-image\")\n",
    "def generate_image(prompt: str):\n",
    "    pixel_array, prmpt = utils.parse_response(utils.query_endpoint(prompt))\n",
    "    image = utils.pixel_to_image(pixel_array)\n",
    "\n",
    "    utils.save_image(\n",
    "        image, filePath=f\"generated_images/{str(datetime.datetime.now())}.png\"\n",
    "    )\n",
    "    img_str = utils.image_to_base64_str(image)\n",
    "\n",
    "    return {\"prompt\": prmpt, \"img_base64\": img_str}\n",
    "```\n",
    "\n",
    "So as not to overwrite the `new.png` over and over again, here I name every new image with the timestamp when it was saved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the Image in Frontend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our backend sends the image data back, let's update the API call handling to:\n",
    "1. extract the image data string from the response JSON\n",
    "2. use the data to update our `imgSrc` state\n",
    "\n",
    "```tsx\n",
    "// App.tsx\n",
    "const onButtonClick = async () => {\n",
    "    setIsLoading(true);\n",
    "    try {\n",
    "      const response = await (\n",
    "        await fetch(`http://127.0.0.1:8000/generate-image?prompt=${prompt}`)\n",
    "      ).json();\n",
    "      console.log(response);\n",
    "      const lastPrompt = response[\"prompt\"];\n",
    "      const imgBase64 = response[\"img_base64\"];\n",
    "      setImgSrc(`data:image/png;base64, ${imgBase64}`);\n",
    "    } catch (err: any) {\n",
    "      // catch any runtime error\n",
    "      console.log(err.message);\n",
    "    } finally {\n",
    "      setIsLoading(false);\n",
    "    }\n",
    "  };\n",
    "```\n",
    "Feeding a base64 image data string as the `src` of an HTML `<img>` tag will render the image just as giving it an image URI. Plus, it saves you one network round trip that you would have needed to fetch the image.\n",
    "\n",
    "Now let's prompt the model to generate a llama family image. And voila!\n",
    "\n",
    "![](images/llama.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "This project was inspired by this amazing [YouTube video](https://www.youtube.com/watch?v=3l16wCsDglU). Salute to the brilliant creator [Nicholas Renotte](https://www.nicholasrenotte.com/)!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
